{
  "cells": [
{
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoBPereira/DeepLearning-UBologna/blob/main/UBologna24_Ex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aofNbODMK2M"
      },
      "outputs": [],
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.8 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 8)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.10 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.10\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaCLRr_uM1iT"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR10 dataset from keras datasets:\n",
        "# https://keras.io/api/datasets/cifar10/\n",
        "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "# This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories.\n",
        "\n",
        "# The load_data() method creates train and test sets.\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(train_images_full, train_labels_full), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "train_labels_full = train_labels_full.squeeze()\n",
        "test_labels = test_labels.squeeze()\n",
        "\n",
        "# Standardize inputs to the interval [0, 1]\n",
        "train_images_full = train_images_full / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# We further divide the original train datasets into train and validation datasets\n",
        "train_images = train_images_full[5000:]\n",
        "valid_images = train_images_full[:5000]\n",
        "train_labels = train_labels_full[5000:]\n",
        "valid_labels = train_labels_full[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1wfnn8bu6EK"
      },
      "outputs": [],
      "source": [
        "# Create a Data Augmentation sub-model\n",
        "\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomRotation(0.1, input_shape=(32, 32, 3)),\n",
        "    layers.RandomContrast(0.1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_HhmhXUPq0X"
      },
      "outputs": [],
      "source": [
        "# Create a simple CNN with the same architecture of the your first CNN\n",
        "# The only difference is that it contains an augmentation module at the beggining\n",
        "# During training, all images go through this module\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "augmented_CNN = keras.models.Sequential([\n",
        "    data_augmentation,\n",
        "    layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', input_shape=[32,32,3]),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, activation='relu',padding='same'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu',padding='same'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bBxU4cFP4xO"
      },
      "outputs": [],
      "source": [
        "# Present a summary of the network architecture\n",
        "# Confirm the architecture and the number of parameters\n",
        "\n",
        "augmented_CNN.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgU59bL4QGsZ"
      },
      "outputs": [],
      "source": [
        "# A NN has to be compiled by calling the compile() method: https://keras.io/api/models/model_training_apis/\n",
        "# Three components have to be defined (recall how backpropagation is appplied during training):\n",
        "# 1. the Optimizer to be used in training\n",
        "# 2. The loss function\n",
        "# 3. The evaluation metric\n",
        "\n",
        "augmented_CNN.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU4IE-yfQMsF"
      },
      "outputs": [],
      "source": [
        "# Training is performed by calling the fit() method: https://keras.io/api/models/model_training_apis/\n",
        "\n",
        "# Hyper-parameters: batch size and epochs\n",
        "# Validation datasets can also be provided\n",
        "\n",
        "# It returns a history object.\n",
        "# Its History.history attribute is a record of training loss values and metrics values at successive epochs,\n",
        "# as well as validation loss values and validation metrics values (if applicable).\n",
        "\n",
        "history = augmented_CNN.fit(train_images, train_labels, batch_size=32, epochs=20,\n",
        "                    validation_data=(valid_images, valid_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqxE-GIHSwqt"
      },
      "outputs": [],
      "source": [
        "# Plot the evolution of the accuracy metrics\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "x = pd.DataFrame(history.history, columns = ['accuracy', 'val_accuracy'])\n",
        "x.plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alJ-O4pxS1u8"
      },
      "outputs": [],
      "source": [
        "# Evaluation the generalization ability of the model\n",
        "# The test set will be used in this step\n",
        "# Classification of a set of examples can be performed using the evaluate() method:  https://keras.io/api/models/model_training_apis/\n",
        "\n",
        "test_loss, test_acc = augmented_CNN.evaluate(test_images, test_labels)\n",
        "print('Test Accuracy: ', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLVkfdPRMCfe"
      },
      "outputs": [],
      "source": [
        "dot_img_file = '/tmp/model_3.png'\n",
        "tf.keras.utils.plot_model(augmented_CNN, to_file=dot_img_file, show_shapes=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
